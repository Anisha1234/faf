#!/usr/bin/python
# Copyright (C) 2012 Red Hat, Inc.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
import pyfaf
import subprocess
import logging
import btparser
import sys

def read_thread_funs(name):
    with open(name, 'r') as f:
        return btparser.Thread(f.read(), True);

# Command line argument processing
cmdline_parser = pyfaf.argparse.ArgumentParser()
cmdline_parser.add_argument("--levels", default="0.3", help="Specify levels to cut (default: 0.3)")
cmdline_parser.add_argument("--distance", default="levenshtein", help="Set distance function used in clustering")
cmdline_parser.add_argument("--stats-only", action="store_true", default=False, help="Print number of clusters and dups with increasing level")
cmdline_args = cmdline_parser.parse_args()

logging.info("Searching local cache for optimized backtraces.")
cache_bugs = pyfaf.run.cache_list_id("rhbz-optimized-backtrace")
logging.info("Found {0} optimized backtraces in local cache.".format(len(cache_bugs)))

logging.info("Reading optimized backtraces.")

thread_names = dict()
threads = []

for (i, bug_id) in enumerate(cache_bugs):
    logging.debug("Reading optimized backtrace {0}/{1} #{2}".format(i + 1, len(cache_bugs), bug_id))
    filename = pyfaf.run.cache_get_path("rhbz-optimized-backtrace", bug_id)
    thread = read_thread_funs(filename)
    thread_names[thread] = bug_id
    threads.append(thread)

distance = cmdline_args.distance
max_cluster_size = 2000

logging.info("Clustering by common function names (maximum cluster size = {0}).".format(max_cluster_size))
funs_clusters = pyfaf.btserver.get_funs_clusters(threads, max_cluster_size, log_debug=logging.debug)
logging.info("Created {0} funs clusters.".format(len(funs_clusters)))

logging.info("Clustering by {0} distance.".format(distance))
dendrograms = pyfaf.btserver.cluster_funs_clusters(funs_clusters, distance, log_debug=logging.debug)

if cmdline_args.stats_only:
    for level in range(0, 101):
        level /= 100.0
        clusters = 0
        dups = 0
        for dendrogram in dendrograms:
            for c in dendrogram.cut(level, 2):
                clusters += 1
                dups += len(c) - 1
        print level, clusters, dups
    sys.exit(0)

logging.info("Removing old clusters.")
pyfaf.run.target_from_name("rhbz-cluster").remove_all()

i = 0
cut_id = 0
for level in map(float, cmdline_args.levels.split()):
    cut_id += 1
    clusters = []
    for (dendrogram, funs_cluster) in zip(dendrograms, funs_clusters):
        clusters.extend([[thread_names[funs_cluster[dup]] for dup in dups] for dups in dendrogram.cut(level, 2)])

    logging.info("Saving {0} clusters at level {1} with {2} bugs.".format(len(clusters), float(level), sum(len(dups) for dups in clusters)) )

    for (j, cluster) in enumerate(clusters):
        logging.debug("Saving cluster {0}/{1} (level = {2} size = {3}).".format(j + 1, len(clusters), level, len(cluster)))
        dups = pyfaf.cache.rhbz_cluster.RhbzCluster()
        i += 1
        dups.id = i
        dups.distance = distance
        dups.cut_id = cut_id
        dups.level = level
        dups.bugs = cluster
        pyfaf.run.cache_add(dups, overwrite=True, target_name="rhbz-cluster")
